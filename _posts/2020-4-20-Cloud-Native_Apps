layout: post
title:  "Cloud Native Applications"
date:   2020-04-20 11:00:00 +0530
categories: Golang
---

# Overview
Cloud Native Applications are designed and implemented to take full advantage of uniqueness of *The Cloud*. They are dynamic, they won't break and they scale arbitrary to work at hyperscale. Continuous delivery and devops are a property of cloud native applications and saves opex.

**Motivation for cloud native apps**
In this section, I will give you an introduction to Cloud native applications and microservices. And this is what we're going to have a look at in this section. First of all, we will talk about some of the motivations for building Cloud native apps in the first place. Next, we will talk about some of the challenges and design principles associated with the concept of Cloud native apps. Then, we will introduce the Cloud native stack with its key technologies. And finally, we will talk about the decomposition of systems and applications using microsservices, so let's get started, then. Motivations for Cloud native apps. So in this video, we are going to look at the following two points. First of all, we will talk about some of the motivations for building Cloud native apps in the first place. And then we will talk about the key three principles of Cloud native applications. So what exactly are Cloud native apps? Cloud native applications are applications that are designed and implemented to take full advantage of the uniqueness of the Cloud. They are dynamic, they won't break, and they scale arbitrarily. So why do we need Cloud native applications? Well, hyerscale. We see a lot more traffic, we see a lot more data, we see a lot more devices, the applications, they have a lot more features, and we need a new type of applications that can handle all this scale. Also, Cloud native applications are usually associated with the term "antifragility", so what does that mean? Well, it means that Cloud native applications don't break anymore. You remember those old MML thick applications, you know if they broke, the system was down. And with Cloud native applications, this is not the case anymore. If it breaks, maybe only certain parts of it break, but not the whole system goes down, and this is what antifragility means. Next thing you usually associated with the Cloud native applications is continuous delivery and devops. So what we want with ever-changing business models is that we deliver new features and new functionality in our applications on a more frequent basis, and this can only be done with Cloud native applications. And then finally, Cloud native applications have something to do with something called Opex savings. Opex is the term for operational expenses. So if you run applications in the Cloud, you usually only pay for those computer resources you actually use, so we don't waste expensive computer resources as you used to do when you had server farms. So what exactly are the three key principles associated with Cloud native applications? Well, they are built and composed as microservices, you've probably heard of that before. They are packaged and distributed in containers, and they are dynamically executed in the Cloud. So this sounds all easy, right, only three key principles. But, Cloud native applications, they also come at a cost, and this is what we're going to cover in the next video, when we will talk about the challenges and design principles associated with the implementation of Cloud native applications, so I hope to see you there.

** Challenges and Design Principles**
So in this video, we're going to take a look at the challenges that come with Cloud native application development, and the design principles of Cloud native apps. So let's get started. So what exactly are the design principles of Cloud native applications? Well first of all, Cloud native apps, they need to be 
designed for performance, they need to be responsive, they need to work on a concurrency, and they need to operate efficiently. Usually this comes at a changed programming model like reactive programming, for example. 
They need to be designed for automation. Remember, one of the motivations for Cloud native applications was the dev ops and continuous delivery. So for this to come true, our applications need to be automated, and they should allow this automation of development tasks and operations tasks. Cloud native applications, 
they need to be designed for resiliency. Remember, anti-fragility was the key word here. Those applications, they should not break. So they need to operate in the face of failure. They need to be fault tolerant and self-healing; 
designed for elasticity. So they need to dynamically scale up and down. Elasticity works in both ways, and they need to be reactive. 
Designed for delivery. Cloud native applications should make sure that you have short development round trips and short feature round trips so it can continuously deliver in small increments, and this delivery needs to be automated, 
and finally, it's designed for diagnosability. Remember, Cloud native applications are usually a massively distributed system. So what you need are cluster-wide logs, traces, and metrics that help you diagnose your systems once things go wrong, and trust me, things will go wrong. 

**Decomposition with Microservices**

Hi and welcome back to Decomposition with Microservices. So, in this video, we're going to take a look at what decomposition using microservices really means and it means components all along the software lifecycle. We will talk about the anatomy of something called Ops components and we'll cover some of the microservice decomposition trade-offs so let's get started then. So, microservices are really components all along the software lifecycle so the usual software lifecycle is you design, you build and you run your applications. So, when you design your applications, well, we used to do this for quite a long time now, we have something called design components. Those design components, there are complexity units, data integrity units, feature units, and decoupled units. We used to do this for quite a long time now. Same goes for build for our development components. Usually those are planning units, knowledge units, development units, and integration units. But now there's something new and those new types of components, they're something called Ops components. We have individual release units, individual deployment units, runtime units, and scaling units and this is something unique to microservices and unique to cloud-native applications. So, I introduced the term Ops component so what exactly is an Ops component? Well, an Ops component is an application that is packaged inside a container and that has several interfaces. It has an interface inbound and outbound that usually talks to some internet protocol like HTTP. It has a starting interface and it has a diagnosis interface. All of this makes an Ops component. But there are some technology-driven constraints of course. Applications should not use the kernel space. They should not listen on random ports. They do not require any exotic operating systems and the used endpoints can be configured. So, what are the microservice decomposition trade-offs then? Well, if you look at the way from how you come from dev components to Ops components, there are several levels, right? So, the one level we all know is where you have one huge dev component, the system, and the Ops components is the usual monolith. You have subsystems that go to macroservices, components to microservices, services to nanoservices. And the further you go down this pyramid, well, the more flexible you are at scale. You have better runtime oscillation. You have independent releases and deployments and usually have high resource utilization but at a cost. The further you go down this pyramid, you have more latency. You definitely have increased infrastructure complexity. You have increased integration complexity and of course troubleshooting your complexity if things go wrong. And the interesting question now is, how can we handle this complexity that comes with the decomposition? And this is what we will cover in the next video when we will introduce the cloud native stack. Hope to see you there.

**Introduction to the cloud native stack*

 Hi, and welcome back. Introduction to the Cloud Native Stack. So, in this video, we're going to take a look at the anatomy of the Cloud native stack, and we will have a look at the Cloud native's landscape with some of this key technologies. Also, we're going to introduce the Cloud native stack used throughout this course, so let's get started then! So what is the Cloud Native Stack? Well, you see its anatomy here, and it's a four-layered architecture. At the very bottom, we have the Cluster Virtualization. This one's concerned with the resources in your cluster, in your cloud, and it decouples everything above it from the physical hardware. On top of the Cluster Virtualizer, there sits the Cluster Scheduler, and the Cluster Scheduler is mainly concerned with containers, and all it does is, it manages the cluster resources and executes individual container on top of those resources. On top of the Scheduler, there's the Cluster Orchestrator, and the Orchestrator is mainly concerned with running whole applications, so it dynamically executes applications on the Cluster, so the Orchestrator talks to the Scheduler, and tells him, "Hey, here's a container to execute for you. Please execute it, and I need three instances of this." So this is all the Orchestrator does, and the Orchestrator and the Scheduler and the Virtualization all together, this can be referred to as the Cluster Operating System, and on top of this Operating System, well, there are your applications. And you have something called an Application Platform, a Cloud Native Application Platform, which you will use to implement your Cloud native applications, and this platform provides runtime environment, and an API for your applications. So you might ask, which technology do I actually use for my Cloud Native Stack? Well, there's good news. You have a lot of choice, and maybe that's also the bad news because you need to make the right choice. You need to combine the right technologies to make a good choice, so if you have a look at the Cluster resources and then Cluster Virtualization, you will find old friends such as docker, or rkt, or runC, Cluster Scheduler, MESOS, docker SWARM, kubernetes, YARN, CoreOS fleet, those are all Cluster Schedulers. Cluster Orchestrators, well, maybe kubernetes here, or Marathon, or AURORA, Chronos, so there are other Orchestrators here to choose from as well. Same goes for the Application Platform. Maybe you implement your Microservices using lagom, or Java, or Spring Cloud, or maybe Go, and this leads me to the Cloud Native Stack we will use throughout this course, and of course, we will use the Go language as an Application Platform, and as the Cluster Operating System, well, we use the Cluster Orchestrator kubernetes, which is also a Cluster Scheduler, and at the very bottom, we use docker for our Cluster Virtualization. So let me summarize, what have we learned throughout this section? Well, first of all, Cloud native applications, they promise superior reliability and arbitrary scalability. We covered some of the important design principles for the development of Cloud native applications. We also talked about certain decomposition trade-offs when using microservices. Remember, nothing is for free. Everything comes at a cost, and the Cloud native stack addresses the inherent challenges and complexities associated with the development of Cloud native applications, and one thing is for sure, Go is a suitable Cloud native application platform, which we are going to see for the rest of this course. 

 